[{"content":"üë®‚Äçüíª I\u0026rsquo;m Jonas, a Software Developer with a keen focus on crafting robust Software Architectures in ‚òïÔ∏è Java. I thrive at the crossroads of business and technology, always keeping a pragmatic perspective on both realms. üëâ My approach is rooted in a combination of proven methodologies, including Extreme Programming (XP), Test-Driven Development (TDD), and Domain-Driven Design (DDD). I believe that these principles pave the way for scalable, efficient, and maintainable software solutions.\n","date":null,"permalink":"/","section":"","summary":"","title":""},{"content":"","date":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":" ‚õµ Navigating the realm of Java, Software Architecture and Testing with in-depth explorations and practical insights. #","date":null,"permalink":"/posts/","section":"Jonas on Software","summary":"","title":"Jonas on Software"},{"content":"How to manage application.yml files in Spring Boot tests without duplication.\nSpring Boot applications rely on an application.yml file to define configuration. Things get a little trickier when writing tests, since both src/main/resources and src/test/resources can contain configuration files. Understanding how Spring Boot handles them will help you avoid duplication and keep test configuration clean.\nHow Spring Boot Resolves application.yml #When running the application normally, Spring Boot loads configuration from src/main/resources/application.yml.\nWhen running tests, the classpath is a combination of main and test outputs, with test classes and resources placed ahead of main ones.\nThis means:\nIf you only have src/main/resources/application.yml, that file will be used during tests as well. If you also define src/test/resources/application.yml, the test version will take precedence. The main version won‚Äôt be loaded at all unless you explicitly import it. So the test application.yml does not ‚Äúmerge‚Äù with the main one, it shadows it.\nOur Definition of A Clean Setup #Our main goal is avoiding duplication. Without a deliberate setup, you often end up copying the entire application.yml into src/test/resources and maintaining two versions side by side. This makes configuration brittle and harder to manage.\nA clean setup should:\nKeep the main configuration as the single source of truth. Allow test-specific overrides without repeating the full configuration. A Clean Setup with Profiles and Imports #To avoid duplicating configuration between test and main, you can make the test configuration explicitly import the main configuration and then layer test-specific overrides on top.\n1. src/test/resources/application.yml\nspring: profiles: active: test config: import: file:src/main/resources/application.yml This setup:\nActivates the test profile whenever tests run, so it will load application-test.yml files. Reuses the main application.yml by importing it. 2. src/test/resources/application-test.yml\nspring: datasource: url: jdbc:h2:mem:testdb jpa: hibernate: ddl-auto: create-drop Here you define overrides that apply only when the test profile is active. Typical use cases include swapping out the database connection, adjusting logging levels, or changing service endpoints.\nWhy This Works Well # Single source of truth: Your main application.yml stays the canonical configuration. Minimal duplication: Test-specific overrides live in one dedicated file. Profile isolation: Tests always run with the test profile, avoiding accidental reliance on production defaults. Explicit precedence: By importing the main config, you make it clear that tests extend it rather than replace it. ","date":"1 September 2025","permalink":"/posts/keeping-spring-boot-test-configurations-simple-and-maintainable/","section":"Jonas on Software","summary":"\u003cp\u003eHow to manage \u003ccode\u003eapplication.yml\u003c/code\u003e files in Spring Boot tests without duplication.\u003c/p\u003e","title":"Keeping Spring Boot Test Configurations Simple and Maintainable"},{"content":"","date":null,"permalink":"/tags/spring-boot/","section":"Tags","summary":"","title":"Spring-Boot"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/testing/","section":"Tags","summary":"","title":"Testing"},{"content":"","date":null,"permalink":"/tags/avro/","section":"Tags","summary":"","title":"Avro"},{"content":"","date":null,"permalink":"/tags/kafka/","section":"Tags","summary":"","title":"Kafka"},{"content":"No bean overrides just serdes configuration\nThere are multiple ways to test Avro publication and consumption in a Spring Boot application.\nFor simplicity, this post will use the terms \u0026ldquo;serializer\u0026rdquo; and \u0026ldquo;serialization,\u0026rdquo; but the same principles apply to \u0026ldquo;deserializers\u0026rdquo; and \u0026ldquo;deserialization.\u0026rdquo;\nTestcontainers #An approach is to replicate your production environment using Testcontainers. You can spin up a Schema Registry container and connect to it in your tests.\nHowever, this adds a fair amount of complexity and overhead, which can often be avoided while still giving you the confidence that your Avro serialization and deserialization work as expected, which is ultimately what you want to test.\nMockSchemaRegistryClient implementation #The kafka-schema-registry-client artifact provides us with a MockSchemaRegistryClient implementation of the SchemaRegistryClient. It works as a stub, keeping all schemas in memory.\nHow to wire the Mock impl. in your Spring-Boot application? #There are numerous ways to let your Spring-Boot application make use of the MockSchemaRegistryClient.\nBrowsing through the source code you would find the following dependency hierarchy.\n[SchemaRegistryClient] ‚ñ≤ ‚îÇ [KafkaAvroSerializer] ‚ñ≤ ‚îÇ [DefaultKafkaProducerFactory] ‚ñ≤ ‚îÇ [KafkaTemplate] The KafkaTemplate bean is autoconfigured by Spring-Boot and depends upon a ProducerFactory which on its turn requires a Serializer.\nThat Serializer in our case is the KafkaAvroSerializer which depends upon a SchemaRegistryClient.\nGiven that setup it only feels natural to use inversion of control to swap in a MockSchemaRegistry for tests and use a CachedSchemaRegistryClient elsewhere.\nThis works fine, but there‚Äôs a catch; if you create your own KafkaAvroSerializer, you‚Äôll have to call its configure() method yourself, otherwise important serialization properties won‚Äôt be set.\nAnd honestly, wiring all of this up by hand feels a bit silly in a Spring-Boot world where things are supposed to be auto-configured for you.\nThe way it was intended #Actually to wire in a Mock implementation you do not need to rely on Spring-Boot as it is provided out of the box by the schema-registry client code.\nAll you have to do is configure your KafkaAvroSerializer with a schema.registry.url property that starts with mock://.\nspring.kafka.properties.schema.registry.url: mock:// The above will make the KafkaAvroSerializer automatically use a MockSchemaRegistryClient.\nHow does mock:// work? #As mentioned earlier, when configure() is called on the KafkaAvroSerializer (which happens automatically by the KafkaProducer), it ends up in SchemaRegistryClientFactory. That factory selects the impl. based upon the given URL as such:\nString mockScope = MockSchemaRegistry.validateAndMaybeGetMockScope(baseUrls); if (mockScope !=null){ return MockSchemaRegistry.getClientForScope(mockScope, providers); } else { return new CachedSchemaRegistryClient(...); } So no extra wiring, no manual serializer config, no hassle.\nMock scopes #The mock:// URL can include a scope, which is a name that links a specific MockSchemaRegistryClient instance to a particular URL.\nFor example, mock://test will create a MockSchemaRegistryClient for the scope named \u0026ldquo;test\u0026rdquo;. Any other producer or consumer configured with the same mock://test URL will share this same client and its in-memory schemas.\nDifferent scopes are useful for:\nsimulating multiple Schema Registry instances isolating test scenarios avoiding conflicts in parallel tests In most cases you can just use mock:// without a scope name, as it defaults to using an empty string (\u0026quot;\u0026quot;) as the scope. This means all components configured with mock:// will also share the same MockSchemaRegistryClient instance.\nA word on the placement of the schema.registry.url property #There are basically 4 location where you can put the property and this goes for all serdes specific properties:\n1. Global Kafka client properties #Applied to all producers, consumers, and Kafka Streams clients.\nspring.kafka.properties.schema.registry.url: mock:// 2. Producer-only properties #spring.kafka.producer.properties.schema.registry.url: mock:// 3. Consumer-only properties #spring.kafka.consumer.properties.schema.registry.url: mock:// 4. Kafka-Stream-specific properties #spring.kafka.streams.properties.schema.registry.url: mock:// ","date":"9 August 2025","permalink":"/posts/testing-avro-serdes-with-spring-boot/","section":"Jonas on Software","summary":"\u003cp\u003eNo bean overrides just serdes configuration\u003c/p\u003e","title":"Testing Avro Serialization and Deserialization with Schema Registry in a Spring Boot Application"},{"content":"How to test a spring based application that uses Kafka?\nThe goal of this post is to share my experience with testing a spring based application that uses Kafka. My aim has always been to have a fast feedback loop when testing my Kafka-based applications.\nI will cover verifying that messages are published to Kafka, consuming them in tests.\nA fully working example project accompanying this post can be found at:\nhttps://github.com/jonas-grgt/spring-kafka-testing-demo\nThe system under test #In this example, I will test a simple application that publishes a FraudSuspected event.\n@PostMapping(\u0026#34;/creditcard/transactions\u0026#34;) void process(@RequestBody CreditCardTransaction transaction) { if (transaction.amount() != null \u0026amp;\u0026amp; transaction.amount().compareTo(new BigDecimal(\u0026#34;10000\u0026#34;)) \u0026gt;= 0) { String fraudId = idGenerator.generateId(); kafkaTemplate.send(fraudAlertTopic, fraudId, FraudSuspected.newBuilder() .setFraudId(fraudId) .setSuspicionReason(SuspicionReason.UNUSUAL_AMOUNT) .setAmount(transaction.amount()) .build()); } } Abstract away your topic configuration #In one of my previous posts I emphasized having a topic naming convention. These fully qualified topic names can get long, convoluted and obnoxious to manage in your tests. In the beginning they can also be subjected to change and in that case I prefer to only have to define or change the topic in one place.\nFor example, when working with the dev.transactions.creditcard.fraud-alert.events topic, I never refer to it by its full name‚ÄîI just call it the \u0026ldquo;fraud alert topic.\u0026rdquo;\nSo, I use configuration shorthand in my configuration:\nmy-app: topics: fraud-alert: ${env}.transactions.creditcard.fraud-alert.events I aim for having one root application.yaml and replace everything that differs per environment, as I did with the env variable above.\nThis way I can use the @Value annotation to inject the topic ‚Äîby reference‚Äî in my tests.\n@Value(\u0026#34;${my-app.topics.fraud-alert}\u0026#34;) String fraudAlertTopic; The startup costs #With the newer Kafka Native images, the container startup time has dropped significantly. That said, the Spring context initialization still tends to dominate the total test boot time. So optimizing how often that context is created becomes critical.\nIn my case for all integration tests I prefer to have one context to rule them all.\nSharing Testcontainers and Spring Contexts #If you let your containers lifecycle be managed by Testcontainers, using the standard @Testcontainers and @Container annotations, you will end up with a new Kafka container for each test class.\nThis forces you to create a new Spring context for each test class as well since the ConnectionDetails will differ between tests.\nTo keep my test suite fast and lean, I optimize for:\na single Spring context shared across all tests a single Kafka container reused by all test classes Let me show you how I set that up.\nSingleton containers managed by spring for the win #When you expose a Container as a Spring Bean, it will be started and stopped automatically by Spring and be active for the lifetime of the application context‚Äî as long as all the tests share the same Spring context.\n@TestConfiguration(proxyBeanMethods = false) class TestcontainersConfiguration { @Bean @ServiceConnection KafkaContainer kafkaContainer() { return new KafkaContainer(\u0026#34;apache/kafka-native:3.8.0\u0026#34;) .withReuse(true) .withExposedPorts(9092, 9093); } } @SpringBootTest @Import(TestcontainersConfiguration.class) class MyKafkaTest { // ... } Notice the @ServiceConnection annotation, it binds the kafka connection details to spring-boot\u0026rsquo;s configuration properties. In other words it will automatically set the spring.kafka.bootstrap-servers etc\u0026hellip;\nVerify records are published #Consumer setup #spring-kafka comes with a handy utility class KafkaTestUtils that allows us to consume messages from a topic in a blocking way. It either waits for an expected number of messages to be received or for a specific timeout to be reached ‚Äîdefault 60 seconds which is a bit too high for me.\nIt does require a KafkaConsumer to be passed in. Luckily, we can leverage the ConsumerFactory that is already autoconfigured by spring-boot.\n@Autowired ConsumerFactory\u0026lt;String, Object\u0026gt; consumerFactory; @Test void myTest() { this.consumer = consumerFactory.createConsumer(\u0026#34;test-group\u0026#34;, \u0026#34;-test-client\u0026#34;); this.consumer.subscribe(List.of(fraudAlertTopic)); ConsumerRecords\u0026lt;String, Object\u0026gt; records = KafkaTestUtils .getRecords(this.consumer, Duration.ofSeconds(5)); } Close the group before leaving the test #Make sure to close the consumer at the end of your test. If you don\u0026rsquo;t, the group remains active, and the next test run using the same group ID will be slowed down due to a rejoin process.\nDuring this delay, you‚Äôll see a log message similar to the following:\n[Consumer clientId=test.bank-test-client, groupId=test-group] (Re-)joining group ‚ö†Ô∏è Avoid closing the consumer group inside the test method, as it won‚Äôt run if the test fails or is interrupted. Instead, use an @AfterEach method to ensure the consumer group is reliably closed after each test. For that you will need to assign it to an instance variable.\n@AfterEach void tearDown() { if (this.consumer != null) { this.consumer.close(); } } Randomize your group ID #Alternatively, you can use a random group ID for each test to avoid the rejoin process. This can be useful when you want to run tests in parallel or when your tests errors before reaching the @AfterEach method. Even with a random group ID, I would still opt to always try to close the group.\nAsync timing issues #Tests like these can be flaky if you\u0026rsquo;re not careful.\nAsync in nature #The actual send to Kafka happens asynchronously, and may be batched depending on the producer configuration. The ListenableFuture returned by KafkaTemplate also reveals this async behavior.\nIf you want to force messages to be sent immediately, in a blocking manner, you can call:\nkafkaTemplate.flush(); In any case, it‚Äôs usually better to embrace the asynchronous nature and use Awaitility to handle timing gracefully:\nAwaitility.await() .atMost(Duration.ofSeconds(5)) .untilAsserted(() -\u0026gt; { ConsumerRecords\u0026lt;String, Object\u0026gt; records = KafkaTestUtils .getRecords(this.consumer, Duration.ofMillis(200)); }); Note that the Duration passed to getRecords is shorter than the Awaitility timeout. This way, if the records aren\u0026rsquo;t available yet, Awaitility will simply retry on the next poll.\nConsumer Group Registration #Kafka initial registration of a consumer group is when Consumer.poll() is first called ‚Äî in our case from the KafkaTestUtils.getRecords() method. If a message is published before that call, and auto.offset.reset is set to latest (the default), the consumer will miss the record entirely.\nThis means that your first test will fail and subsequent tests might succeed depending on how they assert things.\nTo avoid this issue, I set the auto.offset.reset property to earliest in my test application.yml.\n‚ö†Ô∏è Be careful when setting this property in your main or production application.yml as it will affect your production consumers.\nspring: kafka: consumer: properties: auto-offset-reset: earliest An alternative is to create a custom ConsumerFactory for tests only and set the auto.offset.reset property there.\nLenient assertions #Because I use the singleton Testcontainers approach, topics may contain leftover data from previous tests.\nTo avoid flaky tests, I rely on random ids and lenient assertions, meaning don\u0026rsquo;t blindly assert the exact number of messages received, but rather rely on specific ids or shape of the data to ensure you are asserting the records from your test.\nReusable TestContainers #An added benefit of this approach is that I can reuse the container across test runs.\nThe container starts up once, and on subsequent runs it is reused with its existing state. In my case, this isn‚Äôt a problem‚Äîon the contrary, it helps speed things up.\nTo enable container reuse, I set the following in my ~/.testcontainers.properties file:\ntestcontainers.reuse.enable=true Final Test Example # @SpringBootTest @AutoConfigureMockMvc @Import(TestcontainersConfiguration.class) class FraudDetectionTests { @Autowired ConsumerFactory\u0026lt;String, Object\u0026gt; consumerFactory; @Autowired MockMvc mockMvc; @Autowired StubIdGenerator stubIdGenerator; @Value(\u0026#34;${bank.topics.fraud-alert}\u0026#34;) String fraudAlertTopic; private Consumer\u0026lt;String, Object\u0026gt; consumer; @AfterEach void tearDown() { if (this.consumer != null) { this.consumer.close(); } } @Test void fraudSuspectedWhenAmountIsUnusual() throws Exception { // given String fraudId = UUID.randomUUID().toString(); stubIdGenerator.setNextId(fraudId); // when mockMvc.perform(post(\u0026#34;/creditcard/transactions\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(\u0026#34;\u0026#34;\u0026#34; { \u0026#34;amount\u0026#34;: \u0026#34;10000.001\u0026#34; } \u0026#34;\u0026#34;\u0026#34;)) .andExpect(status().isOk()); // then this.consumer = consumerFactory.createConsumer(\u0026#34;test-group\u0026#34;, \u0026#34;-test-client\u0026#34;); this.consumer.subscribe(List.of(fraudAlertTopic)); Awaitility.await() .atMost(Duration.ofSeconds(5)) .untilAsserted(() -\u0026gt; { ConsumerRecords\u0026lt;String, Object\u0026gt; records = KafkaTestUtils .getRecords(this.consumer, Duration.of(200)); assertThat(records) .satisfiesOnlyOnce(record -\u0026gt; { assertThat(record.key()).isEqualTo(fraudId); assertThat(record.value()) .isInstanceOfSatisfying(FraudSuspected.class, c -\u0026gt; { assertThat(c.getFraudId()).isEqualTo(fraudId); assertThat(c.getAmount()).isPresent() .get() .usingComparator(BigDecimal::compareTo) .isEqualTo(new BigDecimal(\u0026#34;10000.001\u0026#34;)); assertThat(c.getSuspicionReason()).isEqualTo(UNUSUAL_AMOUNT); }); }); }); } } Boilerplate #I must admit that there is still some boilerplate code in the tests, something that I\u0026rsquo;ll abstract away in a follow-up post.\n","date":"25 May 2025","permalink":"/posts/kafka-testing-strategies-with-spring/","section":"Jonas on Software","summary":"\u003cp\u003eHow to test a spring based application that uses Kafka?\u003c/p\u003e","title":"Kafka Testing Strategies with Spring"},{"content":"","date":null,"permalink":"/tags/spring-kafka/","section":"Tags","summary":"","title":"Spring-Kafka"},{"content":"Defining, registering, and generating Avro-based schemas with Java and Maven.\nOver the past couple of years, I\u0026rsquo;ve been using Apache Avro as a data format to publish data on Kafka. I\u0026rsquo;ve seen quite a few setups and have come to appreciate one in particular, which I\u0026rsquo;ll share with you.\nThis post comes with a complete example project on GitHub to help you follow along.\nhttps://github.com/jonas-grgt/avro-demo\nProject structure #The project is structured into three Maven modules to promote modularity and reuse. Each can be released as a separate artifact and used independently by consumers ‚Äî or by the producing application itself.\napp containing the application code events containing the schemas and the Java DTOs based on those schemas event-mothers containing my beloved Object Mothers objects for the events The app module naturally depends on events in order to publish events.\nBoth events and event-mothers are released as separate artifacts.\nThis allows consumers to depend on the events module to reuse the generated DTOs without needing to know much about the underlying schema, since it\u0026rsquo;s already embedded in the generated classes (as an inner class $Schema). They can also optionally include the event-mothers module for convenient test data generation.\nAvro Schema Definition #Avro is a data serialization framework that relies on schemas, which are defined in JSON and registered as such in the schema-registry. An alternative, more readable approach that aligns better with how you write regular code, is to use Avro IDL(Interface Definition Language).\nI prefer the IDL format because it provides a more natural way to represent data structures. Just compare the following identical schemas: the first one in JSON and the latter in IDL.\n{ \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;BlogPostCreated\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;title\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;content\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;tags\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;items\u0026#34;: \u0026#34;string\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;status\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;enum\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;symbols\u0026#34;: [ \u0026#34;DRAFT\u0026#34;, \u0026#34;PUBLISHED\u0026#34;, \u0026#34;UNKNOWN\u0026#34; ], \u0026#34;default\u0026#34;: \u0026#34;UNKNOWN\u0026#34; }, \u0026#34;default\u0026#34;: \u0026#34;DRAFT\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;timestamp\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;long\u0026#34;, \u0026#34;logicalType\u0026#34;: \u0026#34;timestamp-millis\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;properties\u0026#34;, \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;PostProperties\u0026#34;, \u0026#34;fields\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;author\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;category\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;language\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } ] } } ] } enum Status { DRAFT, PUBLISHED, UNKNOWN } = UNKNOWN; record PostProperties { string author; string category; string language; } record BlogPostCreated { string title; string content; array\u0026lt;string\u0026gt; tags; Status status = \u0026#34;DRAFT\u0026#34;; @logicalType(\u0026#34;timestamp-millis\u0026#34;) long timestamp; PostProperties properties; } Java Class Generation #Avro provides a Maven plugin to generate Java classes from Avro JSON schemas and Avro IDL files.\nThe following snippet configures the Maven plugin to generate Java classes from Avro IDL files (.avsc). It generates them into the target/generated-sources directory based on the events.avdl file located in src/main/resources.\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.avro\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;avro-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${avro-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;idl-protocol\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!-- by default all *.avdl files are picked up from the src/main/resources directory --\u0026gt; \u0026lt;sourceDirectory\u0026gt;src/main/resources\u0026lt;/sourceDirectory\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.directory}/generated-sources \u0026lt;/outputDirectory\u0026gt; \u0026lt;!-- use java.lang.String instead of org.apache.avro.util.Utf8 --\u0026gt; \u0026lt;stringType\u0026gt;String\u0026lt;/stringType\u0026gt; \u0026lt;!-- use java.math.BigDecimal for bytes with decimal logical type --\u0026gt; \u0026lt;enableDecimalLogicalType\u0026gt;true\u0026lt;/enableDecimalLogicalType\u0026gt; \u0026lt;!-- generate getters that return java.util.Optional --\u0026gt; \u0026lt;gettersReturnOptional\u0026gt;true\u0026lt;/gettersReturnOptional\u0026gt; \u0026lt;!-- only use Optional for fields that are nullable in the schema --\u0026gt; \u0026lt;optionalGettersForNullableFieldsOnly\u0026gt;true \u0026lt;/optionalGettersForNullableFieldsOnly\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; The Schema Registry does not support Avro IDL directly ‚Äî it only accepts Avro JSON (AVSC) schemas. #As discussed in a previous post, before taking any technical steps to register schemas, you need to decide whether to manage them in a centralized or distributed manner.\nSince schemas are part of the source code, I prefer to manage them in a distributed way using the kafka-schema-registry-maven-plugin.\nHowever, there\u0026rsquo;s a problem: the kafka-schema-registry-maven-plugin only works with JSON schemas ‚Äî not IDL files.\nü§î So how do we get the Avro IDL files registered in the schema-registry?\nmaven-avdl-to-avsc-plugin #The maven-avdl-to-avsc-plugin converts Avro IDL files to Avro JSON (.avsc) files, which can then be registered with the schema registry.\nYou just need to point it to your Avro IDL files and specify the output directory.\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.jonasg\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;avdl-to-avsc-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${avdl-to-avsc-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;avdl-to-avsc\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;avdlDirectory\u0026gt;${avdl.dir}\u0026lt;/avdlDirectory\u0026gt; \u0026lt;avscDirectory\u0026gt;${project.build.directory}/generated-sources/avsc \u0026lt;/avscDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Registering the Avro Schemas #Now that the Avro IDL files have been converted to JSON, we can register them with the schema registry:\n\u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;dev\u0026lt;/id\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.confluent\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-schema-registry-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${kafka-schema-registry-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;schemaRegistryUrls\u0026gt; \u0026lt;param\u0026gt;${schema.registry.url}\u0026lt;/param\u0026gt; \u0026lt;/schemaRegistryUrls\u0026gt; \u0026lt;userInfoConfig\u0026gt; ${schema.registry.api.key}:${schema.registry.api.secret} \u0026lt;/userInfoConfig\u0026gt; \u0026lt;subjects\u0026gt; \u0026lt;dev.orders\u0026gt; target/generated-sources/avsc/Order.avsc \u0026lt;/dev.orders\u0026gt; \u0026lt;/subjects\u0026gt; \u0026lt;compatibilityLevels\u0026gt; \u0026lt;dev.orders\u0026gt; FORWARD_TRANSITIVE \u0026lt;/dev.orders\u0026gt; \u0026lt;/compatibilityLevels\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Note that there is no one-to-one mapping between the Avro IDL files and the generated JSON. Behind the scenes every avro record and enum is converted to its own avsc JSON file.\nThis means when you have one big AVDL file, it will be split into multiple smaller AVSC files, and you will need to refer to those avsc files when registering schemas.\nTo register schemas and set their compatibility level effectively (ideally in a CI pipeline), you can use:\nmvn package schema-registry:register -pl events -P dev -ntp mvn package schema-registry:set-compatibility -pl events -P dev -ntp A little caveat #The maven-avro-plugin will generate a JSON schema (inside the generated Java class) that , during serialization, will be compared to the already registered schema.\nThe caveat is that, if there are differences between the schemas, serialization will fail with an exception.\nBecause we configured the maven-avro-plugin to use java.lang.String instead of org.apache.avro.util.Utf8 as the string type. This information is captured within the Schema inside the generated class as such:\n{ \u0026#34;type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;avro.java.string\u0026#34;: \u0026#34;String\u0026#34; } } Yet this information is not inside the JSON schema generated by the avdl-to-avsc-maven-plugin because it just translates AVDL to AVSC.\nFor that reason the serialization will fail, and you will be presented by a very confusing schema not found exception when using the kafka-avro-serializer to serialize the object.\nIf we could remove this extra metadata from the Java-generated schema, the two schemas would become identical. While it\u0026rsquo;s not possible during class generation, it is possible during serialization ‚Äî by setting the following property:\nspring: kafka: properties: avro.remove.java.properties: true This instructs the serializer to strip Java-specific metadata (like avro.java.string) from the schema before comparing it to the one registered in the Schema Registry.\n","date":"16 May 2025","permalink":"/posts/avro-schemas-with-kafka-and-java-my-practical-workflow/","section":"Jonas on Software","summary":"\u003cp\u003eDefining, registering, and generating Avro-based schemas with Java and Maven.\u003c/p\u003e","title":"Avro Schemas Generation and Registration with Kafka and Java: My Practical Workflow"},{"content":"How to name and manage topics.\nThe success of Event-Driven Architecture (EDA) isn‚Äôt just about getting the technology right, it‚Äôs also about how teams work together, both within their own boundaries and across organizational lines. It‚Äôs about shared models built through meaningful naming, defined ownership, and a mutual understanding of what each topic represents.\nEDA introduces powerful decoupling between systems, but that decoupling also demands alignment. In this post, we‚Äôll focus on two critical pillars of that alignment: topic naming and ownership.\nOrganizational Considerations #Topic Naming Convention # There are 2 hard problems in computer science: cache invalidation naming things\nAs your system grows to dozens or even hundreds of topics, a clear and consistent naming convention becomes more than just a best practice, it becomes the foundation for a shared understanding.\nA well-defined Topic Naming Convention, captured in an Architectural Decision Record (ADR), ensures that everyone knows what a topic represents, what kind of data it carries, and how it fits into the broader model. It\u0026rsquo;s a small investment that pays off in alignment, clarity, and long-term maintainability.\nName topics like you would name your domains (DNS) #DNS provides an excellent model for structuring topic names. Just like domain names, topics could follow a hierarchical approach, moving from a broad scope (e.g., .com) to a more specific one, step by step.\nThis makes DNS-style naming a great foundation for a topic naming convention:\nStart with the broadest category and progressively narrow down. Use dots to separate different levels of specificity. Domain Driven Design #This approach aligns well with Domain Driven Design (DDD) principles, where you can start with the bounded-context and then move to the subdomains all to the way down to the aggregate root.\nExample Naming Conventions: # E-commerce: prd.orders.in-store.checkout.events prd.orders.online.checkout.events prd.orders.online.refund.state dev.products.inventory.events dev.customers.profile.events IoT dev.iot.sensors.temperature.room.events Banking: dev.transactions.creditcard.fraud-alert.events Environment Prefix #Even if you operate with a dedicated Kafka cluster per environment (e.g., dev, staging, prod), it\u0026rsquo;s still a good idea to include an explicit environment prefix in your topic names.\nThis small redundancy helps eliminate ambiguity and protects against costly mistakes like, accidentally producing or worse, deleting topics from the wrong environment.\nManaging Topics #Now we know how to name things it is time to discuss how to actually create those topics.\nThe Question of Ownership #Who actually owns, or, put differently, takes responsibility for the full lifecycle of a topic? In many organizations, this responsibility falls on a DevOps or infrastructure team. Often this stems from the fact that they are the ones who in the end are responsible for the cluster.\nHowever, this seems counterintuitive when compared to a more familiar business application resource: the database table. Most teams manage their own database tables, using automated tools like Flyway or Liquibase within a deployment pipeline or at startup. Yet, when it comes to (Kafka) topics, this ownership model is rarely followed. Why is that?\nThe ultimate question is, who should be responsible for the full lifecycle of a topic?\nTo answer this question, you need to consider between two different approaches: Centralized Approach VS. Distributed Approach.\nCentralized #In a centralized model, topic management is handled through a single repository. Naturally, this repository brings up all kinds of wanted or unwanted coupling concerns.\nüß∑ Organizational Coupling: Changes to topics must go through a central team (like DevOps or Platform Engineering), introducing a process bottleneck. Application teams become dependent on this team for changes, which slows down development and increases friction and effectively creating organizational silos.\nüöÄ Deployment Coupling: Changes to topics need to be deployed together with the application code, which can lead to increased complexity and coordination overhead during deployments.\nSummarized # ‚úÖ Easier to enforce company-wide rules. ‚úÖ Less complex when dealing with a small number of teams or services. ‚ùå Creates a bottleneck if the central team is unaware of specific use cases. ‚ùå Requires release coordination, making changes more convoluted and risky. ‚ùå Does not scale well as the number of teams and services grows. Distributed Approach #In a distributed model, topics are logically coupled to the code that uses them, allowing each team to manage their own topics.\nüß∑ Ownership and Autonomy: Each application team has full ownership and responsibility for the topics which they are master from and as such they produce on. This fosters autonomy, allowing teams to tailor topic configurations to their specific needs and iterate faster without depending on a central authority.\nüöÄ Decoupled Deployments: Topic changes are deployed as part of the application deployment lifecycle. This reduces the need for cross-team coordination and the risk of infrastructure changes blocking application releases. Teams can evolve their topics alongside their application code.\n‚úÖ Teams have full ownership and autonomy over their topics. ‚úÖ Reduces dependencies on a central team, improving development speed. ‚úÖ Allows for greater flexibility and customization based on specific needs. ‚úÖ Scales well as the number of teams and services grows. ‚ùå Can lead to inconsistencies in naming conventions and configurations across teams. ‚ùå Requires each team to develop and maintain expertise in Kafka topic management. ‚ùå Makes it harder to enforce company-wide policies and standards consistently. Each approach has its trade-offs, and the right choice depends on your organization‚Äôs size, structure, and priorities.\nA successful Event-Driven Architecture isn‚Äôt just about choosing the right tools, it\u0026rsquo;s about aligning technical practices with team ownership and organizational structure. Whether centralized or distributed, what matters most is making the trade-offs explicit and intentional.\n","date":"17 April 2025","permalink":"/posts/kafka-without-structure-is-just-kafka/","section":"Jonas on Software","summary":"\u003cp\u003eHow to name and manage topics.\u003c/p\u003e","title":"Kafka Without Structure Is Just... Kafka"},{"content":"","date":null,"permalink":"/tags/maven/","section":"Tags","summary":"","title":"Maven"},{"content":"Tips and tricks to move swiftly on the command line.\nI grew up using the command line and have come to enjoy its efficiency and power. It often pains me to see colleagues struggling with it, using methods that seem cumbersome to me. Occasionally, they notice how swiftly I navigate the command line and ask what magic Oh My Zsh plugin I\u0026rsquo;m using. The truth is, it\u0026rsquo;s just the good old GNU Readline, a tool created in 1989. Yes, it\u0026rsquo;s that old!\nReadline #Readline has two editing modes: emacs and vi, with emacs being the default mode and the one we will focus on.\nBash makes use of Readline, while Zsh uses ZLE (Zsh Line Editor), which has similar capabilities. So as long as you are using either of these shells, you should be good.\nSo let\u0026rsquo;s get moving!\nTools setup #Before we get started, let\u0026rsquo;s make sure our tools are set up correctly. I am going to focus on Mac and the tools iTerm2 and IntelliJ.\nMeta-key #The Meta key is a modifier key on certain keyboards that was originally found on early computer systems like the Lisp Machine and the Knight keyboard. Its function is similar to the Alt key on modern keyboards. In many software applications, including text editors and command-line interfaces, the Meta key is used to provide additional keyboard shortcuts.\nOn today\u0026rsquo;s keyboards you won\u0026rsquo;t find a Meta key, but you can use the Option (mac) key. It just needs to be configured correctly.\niTerm2 # Intellij # Shortcuts #Movement # Ctrl-a: Move to the beginning of the line. Ctrl-e: Move to the end of the line. Ctrl-b: Move backward one character. Ctrl-f: Move forward one character. Alt-b: Move backward one word. Alt-f: Move forward one word. Ctrl-l: Clear the screen, reprinting the current line at the top. Editing # Ctrl-d: Delete the character under the cursor. Ctrl-h or Backspace: Delete the character to the left of the cursor. Alt-d: Delete the word after the cursor. Alt-Backspace: Delete the word before the cursor. Ctrl-_ or Ctrl-x Ctrl-u: Undo the last editing command. Ctrl-k: Cut the text from the cursor to the end of the line. Ctrl-u: Cut the text from the cursor to the beginning of the line. Ctrl-w: Cut the word before the cursor. Ctrl-y: Paste (yank) the last killed text. History # Ctrl-r: Reverse search through history. Ctrl-p: Previous command in history. Ctrl-n: Next command in history. ","date":"13 January 2025","permalink":"/posts/swift-strokes/","section":"Jonas on Software","summary":"\u003cp\u003eTips and tricks to move swiftly on the command line.\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eSwift Strokes\u003c/span\u003e\u003cbr/\u003eMoving along the command line"},{"content":"","date":null,"permalink":"/tags/command-line/","section":"Tags","summary":"","title":"Command-Line"},{"content":"","date":null,"permalink":"/tags/shell/","section":"Tags","summary":"","title":"Shell"},{"content":"","date":null,"permalink":"/tags/unix/","section":"Tags","summary":"","title":"Unix"},{"content":"TL;DR of the Test First series.\nAs we conclude the Test First series, I\u0026rsquo;d like to revisit and emphasize the key takeaways from our discussions.\nTest Driven Development (TDD) is a practical tool that, when understood and applied correctly, within the right context, can be highly beneficial. TDD isn\u0026rsquo;t about adhering to a strict dogma, or looking down on those who haven\u0026rsquo;t adopted it;\nTDD is fundamentally about ensuring that your codebase remains robust against regressions during:\nRefactorings Introduction of new features Modifications to existing features TDD achieves this by playing a crucial role in the design of our applications. Once we have developed a working feature, regardless of its initial code quality or elegance, having robust tests in place enables us to modify the internal workings with confidence. This is what the whole red-green-refactor cycle is about.\nSuch an approach ensures that changes made to improve or optimize the code (after our initial green tests or even months later when refactoring or altering the feature\u0026rsquo;s behavior) do not inadvertently introduce errors or regressions.\nDo not confuse a working feature by working code. A feature is implemented by code, but the code itself is not the feature nor the goal of what should be tested.\nTo effectively achieve this, it\u0026rsquo;s crucial to write tests at an appropriate level of abstraction. But what is the right level of abstraction?\nStop wasting time Unit Testing #A Unit Test, as all to often misunderstood as a method or class test, is definitely not the right level of abstraction to form a proper foundation of any test suite. For that reason, the classical testing pyramid has become obsolete.\nThe foundation of your test codebase, if based on testing each layer, class, or method in isolation, tends to be counterproductive. This approach can result in a fragile codebase, one that is prone to failure with the slightest changes, rather than fortifying it against regressions.\nAt the heart of your testing approach should be a focus on behavior, followed by implementation details. Grasping this distinction within your own codebase is a significant aspect of test architecture. Understanding that tests should primarily verify the behavior of the system ‚Äî how it reacts and what it does ‚Äî and then consider the specific ways these behaviors are implemented, is key. This approach ensures that tests remain relevant and valuable, even as the implementation evolves over time.\nArchitecture also for our tests #It\u0026rsquo;s vital to treat our test codebase with the same seriousness as our production code architecture. Planning and understanding the rationale behind the structure of the test suite are essential. This clarity should extend to all project participants, ensuring a cohesive and effective testing strategy.\nTest First #Writing tests before production code encourages thoughtful consideration of the system\u0026rsquo;s requirements and behaviors. It also puts the focus on testing architecture - figuring out how to test the features - enhances the process. In contrast, starting directly with production code can turn these considerations into hurdles.\nA Test-first mindset aims to enable the creation of better, more change-resilient production code, by integrating testing as a fundamental part of the development process, if conducted at the right level of abstraction.\nIn the end, our tests are more important than our production code. Because without tests, we can\u0026rsquo;t be confident to change our production code and in doing so being able to move our software products forward at a steady peace.\n","date":"17 March 2024","permalink":"/posts/test-first-tldr/","section":"Jonas on Software","summary":"\u003cp\u003eTL;DR of the Test First series.\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eTest First Ôºç Part 5\u003c/span\u003e\u003cbr/\u003e TL;DR"},{"content":"","date":null,"permalink":"/tags/tdd/","section":"Tags","summary":"","title":"TDD"},{"content":"Your tests also deserve some architecting\nTL;DR #Partition first by behavior, then by architectural layers\nArchitecture plays a critical role at all levels of the software development process, yet one level often gets overlooked.\nTypically, our architectural efforts are focused on production code, often overlooking a vital component: the test code. To maximize the benefits of your test suite, integrating your test design into your overall architectural decisions is essential.\nYour tests also deserve some architecting #The test suite is often treated with an indifferent attitude, as if its structure and design are of minor importance. Even when somewhat architecturally cared for, it seldom extends beyond dictating a certain percentage of code coverage, which on its own is a poor metric of quality.\nA thoughtfully designed test suite is essential for the sustained success of any application, facilitating continuous and steady advancement of the production code base. It also contributes to a coherent and comprehensible base for future changes. These benefits are frequently underestimated.\nA testing architecture must provide clear guidelines on several essential aspects:\nLevels and Types of Tests: specify which types of tests are conducted at different levels of the application and explain the reasoning behind these choices. Test Data Management: define strategies for setting up and tearing down test environments, which could include using patterns like Test Object Managements using, i.e. Object Mothers Spring Context Management. databases, message-brokers and other infrastructural components data setup and clean-up strategies. Reusable Components and Extensibility: Identify available reusable components, such as standardized fixtures or helper methods. Documenting these and other resources promotes efficiency, reduces redundancy, and fosters uniformity across the testing code base.\nThe right level of partitioning #Software architecture abstracts an application into layers or components, each with a dedicated responsibility What often happens is that these layers are used to partition your tests by a specific type of test.\nThis line of thinking leads to a brittle test code base that breaks down upon every refactoring.\nEven worse is when all layers are tested individually in isolation completed by one big integration test. Ever saw a mapper being tested in isolation? That\u0026rsquo;s a clear sign of this anti-pattern.\nIf not a mapper-test then maybe a test like this looks recognizable?\nclass PersonService { public void createPerson(Person person) { personRepository.save(person); } } // Test Code var personRepository = mock(PersonRepository.class); var personService = new PersonService(personRepository); var person = PersonMother.male(); personService.createPerson(person); verify(personRepository).save(person); At first glance, this test seems reasonable ‚Äì it checks if PersonService calls save on personRepository. However, this test is inherently flawed as it tightly couples the test to a specific implementation detail of PersonService. The issue here is that if the implementation of createPerson changes, even if the overall behavior remains the same, the test may fail.\nThis creates a fragile testing environment, where tests break due to implementation changes rather than actual bugs or behavioral changes.\nStill not convinced? Then you probably misunderstood what is meant by behavior.\nBehavior once more #By behavior, I\u0026rsquo;m referring to the overall functionality of the application as it relates to business rules and user interactions, not the details of code, methods, or classes. This behavior is the practical manifestation of what the application does, making the code itself a secondary, supporting detail.\nTo build more robust tests, it\u0026rsquo;s vital to partition tests based on behavior before partitioning them based on architectural layers. Partitioning based on architectural layers is not wrong perse, but it should not be the primary partitioning strategy.\nBy focusing on what the application is supposed to do, rather than how it does it (its implementation), tests become more resilient to changes in your codebase.\n","date":"10 March 2024","permalink":"/posts/architecture-is-not-only-for-production-code/","section":"Jonas on Software","summary":"\u003cp\u003eYour tests also deserve some architecting\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eTest First Ôºç Part 4\u003c/span\u003e\u003cbr/\u003e Architecture is not only about production code"},{"content":"Looking at an Ancient Structure\nTL;DR #The classical testing pyramid focuses on the cost of writing, running, and maintaining tests, which is a narrow and single-minded perspective not adapted to today\u0026rsquo;s technology landscape.\nIn the second part of this series, I stress the importance of focusing on testing actual requirements, behavior, use-cases, and not implementation details like methods and classes.\nYet things aren\u0026rsquo;t merely as simple as that. Through shedding new light on the classical testing pyramid, this article will challenge long-held testing dogmas, advocating for a paradigm shift that better aligns with the complexities and nuances of contemporary software ecosystems\nA short history lesson #Most developers are familiar with the concept of the testing pyramid, introduced by Mike Cohn in his book \u0026ldquo;Succeeding with Agile: Software Development Using Scrum\u0026rdquo; back in 2009. It suggests an ideal test distribution:\nA solid base of unit tests A middle layer of integration tests A smaller top layer of end-to-end tests The pyramid emphasizes a particular aspect of test type distribution, primarily based on the cost ‚Äî in terms of time and money ‚Äî associated with writing, running, and maintaining tests. However, it solely considers this aspect of cost distribution, overlooking other valuable factors that should also be taken into account. Unit Tests are a Weak Foundation #In today\u0026rsquo;s software development world, the testing pyramid\u0026rsquo;s foundation isn\u0026rsquo;t as sturdy as it once was. It\u0026rsquo;s built on the idea that having many unit tests equals a well-tested application, but this can be highly misleading. As we discussed before, the ambiguous nature of unit testing in our industry typically guides us to write numerous isolated tests, all while mocking out external dependencies. This practice eventually leads to a brittle test codebase.The focus on technical distribution steers us away from the real goal of testing behavior.\nIn essence, the classical testing pyramid tells us how to test things, but it\u0026rsquo;s not all that relevant because it doesn\u0026rsquo;t help us:\nprevent regression document our software refactor freely and evolve our applications at a steady pace, all of that when making changes without altering requirements Honeycomb to the rescue #With the exponential growth of the web came the need for microservices, the evolution of architectural patterns, new types of databases, queues, and the ever-increasing speed and power of our computers. We are no longer creating the same applications as we did in the past. I argue that these shifts have undoubtedly influenced the way we write tests.\nOver the past decade, there has been a noticeable shift towards a multitude of leaner services, moving away from monolithic applications. These modern services are often simplified, with a reduced business responsibility, while the emphasis has shifted towards integration with other services and infrastructural components.\nAdvancements in CPU cycles and virtualization have propelled us into an era where entire databases can be spun up in a matter of seconds when running tests, thanks to tools like Testcontainers.\nAs a response to these changes, the testing honeycomb model has emerged, placing greater emphasis on integration tests and placing less focus on implementation details and integrated tests.\nA Honeycomb++ #From today\u0026rsquo;s perspective, I recognize increased value in the testing honeycomb, positioning integration tests as the cornerstone of a modern testing approach. I would like to propose a minor adjustment, shifting focus from the type of test to the true value of testing ‚Äì in other words, emphasizing the importance of testing behavior. Spotlight On Behavior Testing #At its core, you test the behavior of your system through various means such as unit, integration, slice, component, or any other type of test required.\nüëâ A behavior test says nothing about the type of test, being unit or integration. This choice, or rather architectural decision, is yours to take and varies for each context.\nImplementation detail #It is important to state that testing implementation details, even on a class or method level, is not wrong and at times even necessary, especially for complex logic within a component.\nüëâ However, it should never, like the classical testing pyramid implicitly proposes, be the foundation of your test distribution.\nTo admit, it\u0026rsquo;s not always clear what actually is an implementation detail test. This is something that should be formulated for every project and is part of the architecture of your application.\nTesting implementation details means focusing on the specific, internal workings of a piece of software, such as the methods, classes, and internal state and flow, rather than on its overall behavior or the outcomes it produces.\nInternal Focus: These tests concentrate on the internal structure of the code, rather than on what the code does from a behavioral perspective.\nSusceptibility to Change: Tests that focus on implementation details are often fragile and prone to failure with code refactoring or internal changes, even when the external behavior remains consistent.\nState Verification: They frequently involve verifying the internal state of an object or the interactions between internal components, rather than the end result or output of a process.\nIt is easy to fall into the trap of over-testing simple orchestration code. This occurs when tests are written for code that merely coordinates or orchestrates calls to other components. For instance, testing an isolated Spring MVC controller or service might seem beneficial, but if these tests only assess how the code is structured internally, rather than how it behaves or interacts with other layers within the system, they may be focusing too much on implementation details.\nIntegrated #These tests are conducted in actual environments to identify integration failures, and that can even be in production. Often, our system landscape includes a variety of microservices, databases, queues, and other infrastructure components. These are orchestrated by a complex mix of release and deployment pipelines, comprising scripts, infrastructure-as-code definitions, and configuration files. Collectively, they form a functioning end-to-end system.\nAn integrated test examines the entire landscape in an actual environment, akin to a smoke test.\nTo offer a more concrete example: in a recent project, I worked with Kafka, where AVRO messages were published. This process required a pre-existing schema in a component known as the schema-registry. Frequently, someone would forget to add or update the schema in our Terraform repository.\nAll tests locally were succeeding, and even upon deployment to the dev environment, everything seemed up and running. Until our automated test fired some calls against the system that caused it to fail because of the missing schema. This could indeed have been captured by a manual test, yet there is great value in automating things.\nWhat does an integrated test it look like? #It can be as simple as a set of shell or python scripts firing off some requests against the system and waiting for a certain response. Something that can be run after each deployment in an environment.\nMapping the cost and duration on the Honeycomb #When we map the cost and duration of tests onto the honeycomb model, a more balanced distribution emerges. With the behavior tests being the most balanced as they can be both slow and fast at the same time. There is a cost to implementation detail tests, as they are susceptible to break upon refactoring. Integrated tests, on the other hand, represent the highest expense in both execution and maintenance, due to their complex nature and the ongoing challenge of upkeep.\nüëâ In essence, when all behavior is thoroughly tested, it often results in most of the code being effectively covered.\n","date":"5 February 2024","permalink":"/posts/relevance-of-the-classical-testing-pyramid/","section":"Jonas on Software","summary":"\u003cp\u003eLooking at an Ancient Structure\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eTest First Ôºç Part 3\u003c/span\u003e\u003cbr/\u003e On the Relevance of the Classical Testing Pyramid"},{"content":"On the Misconceptions Surrounding the Unit in Unit Testing\nTL;DR #The unit you want to test is not a class nor a method but a behavior.\nIn the first part of this series, I stress the importance of prioritizing a well-designed and overall reliable test suite over strict adherence to the TDD mantra. However, defining what constitutes a well-designed and reliable test suite raises the question of what should be tested, or more precisely, what the unit under test is.\nIf you were to ask a group of developers to give a definition of a Unit Test, you\u0026rsquo;d likely receive a plethora of different responses. Likewise, within the industry, there is a lot of ambiguity around what a Unit Test exactly is. We simply cannot agree on a common concept what the unit exactly represents.\nIn the second part of this testing series, I aim to clarify this ambiguity by defining what actually should be tested or what the unit actually is. This will pave the way for a deeper understanding of the true value of writing tests.\nNot The Smallest Testable Module #Unfortunately, the term \u0026ldquo;Unit Testing\u0026rdquo; has become synonymous with testing overall. Conversely, a common definition of Unit Testing is the smallest testable component that can be isolated1. For many, this translates to testing a single public method or class, leading to the pitfalls of brittle test suites and misconceptions about testing and difficulties with TDD.\nUnit Tests: A Source of Brittleness #Test code bases composed of numerous small tests focused on methods and classes often suffer from brittleness. A brittle test suite breaks with even the slightest code alterations unrelated to changing requirements. This becomes particularly evident when routine code refactors, such as adding a dependency or a parameter, result in test failures.\nIf your test code base fractures in response to minor alterations without changes in requirements, it\u0026rsquo;s a clear indication of a poorly designed test suite. Awareness of this issue may be lacking, especially if code coverage numbers present a seemingly positive picture. However, focusing solely on code coverage provides a narrow perspective on the overall health of your test code base.\nAdditionally, these types of test suites fall short in narrating the essence of the application. Instead of conveying the domain, they tend to focus on technical implementation details. Similar to the concept of screaming architecture, tests should vividly scream what the application is about.2\nThe fragility and lack of documentation in tests can be attributed to an excessive emphasis on testing code in isolation. However, it prompts the question: what else should be tested if not the code we just wrote or want to write?\nA pinnacle of brittle tests focused on a technical implementation detail #Admittedly, a very simplistic example, yet I have already encountered tests that look almost exactly like this:\nclass PersonService { public void createPerson(Person person) { personRepository.save(person); } } var personRepository = mock(PersonRepository.class); var personService = new PersonService(personRepository); var person = PersonMother.male(); personService.createPerson(person); verify(personRepository).save(person); The Test Trigger # The trigger for a test is not a class nor a public method Ôºç Ian Cooper Ôºç TDD where did it all go wrong3\nFor an extended period, adding a new class or method was exactly what was driving my tests. Ironically, it was this idea on testing that made TDD comprehensible for me.\nThis commitment found additional reinforcement through the abundance of straightforward examples and tutorials available on writing tests and TDD, all consistently emphasizing explicit code testing.\nI believe that one should not test code explicitly, most of the time. So then naturally, the question arises, what should be tested explicitly if not code?\nBehavior First #Today I no longer let the creation of a method or class drive my tests. Instead, I focus on a fundamental question: why do I write code? I write code because there is always a requirement for a system to behave in a certain way. And that I try to capture in my tests.\nThis perspective should guide testing most of the time. Tests should stem from the requirements expected to be implemented, how the system should behave and not the methods and classes implementing those requirements.\nExplicitly Test Behavior To Implicitly Test The Code Covering That Behavior\nBy explicitly testing the expected behavior of your application, you will implicitly test almost all of your code. Any portions left untested by this method are likely unrelated to expected behaviors, relate to exceptional cases, such as checked exceptions, or configuration classes. As a consequence, your code coverage should be naturally high.\nI emphasized the significance of letting behavior guide your testing in the majority of cases. However, are there instances when this may not be the optimal approach? This question will be answered in the upcoming third installment of my Test First Series, where we go back in time to take a look at the classical Testing Pyramid.\nWikipedia (2023) https://en.wikipedia.org/w/index.php?title=Unit_testing\u0026oldid=1186674931\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nScreaming Architecture (2021) https://blog.cleancoder.com/uncle-bob/2011/09/30/Screaming-Architecture.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTDD where did it all go wrong (2017) https://youtu.be/EZ05e7EMOLM?si=2M-K1F0O53EYChU0\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"27 December 2023","permalink":"/posts/unit-test-ambiguity/","section":"Jonas on Software","summary":"\u003cp\u003eOn the Misconceptions Surrounding the Unit in Unit Testing\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eTest First Ôºç Part 2\u003c/span\u003e\u003cbr/\u003e The Unit Test Ambiguity"},{"content":" üé§ #","date":null,"permalink":"/talks/","section":"My Talks","summary":"","title":"My Talks"},{"content":"Discover how the Object Mother concept empowers developers to effortlessly generate intricate test objects, enhancing code readability, maintainability, and overall testing efficiency.\nThe creational problem #A good structured test exists out of three parts commonly known as: Given When Then or Arrange Act Assert.\nWithin the Given part you declare a set of objects that will drive your test. Initially the creational logic is simple, as the objects may have fewer fields or lack coherence. However, inevitably over time, the set of objects grows as their intricacy, they become more complex and time-consuming to construct.\nVarious solutions exist to address those issues, such as Test Data Factories, Data Fakers, Test Data Generators or Fixture Builders etc .. In the end they all share a common goal: simplifying object creation while ensuring reusability. Martin Fowler has even coined a term for this concept, known as Object Mother.\nLet\u0026rsquo;s delve deeper into the problem by examining a concrete example. While this example may not be overly complex or entirely realistic, the purpose of this article is to work with practical scenarios without getting bogged down in the intricacies of object creation.\nAddress billingAddress = new Address.Builder() .streetAddress(\u0026#34;123 Main St\u0026#34;) .city(\u0026#34;Anytown\u0026#34;) .state(\u0026#34;CA\u0026#34;) .zipCode(\u0026#34;12345\u0026#34;) .country(Country.US) .build(); Address shippingAddress = new Address.Builder() .streetAddress(\u0026#34;456 Oak Ave\u0026#34;) .city(\u0026#34;Othertown\u0026#34;) .state(\u0026#34;CA\u0026#34;) .zipCode(\u0026#34;67890\u0026#34;) .country(Country.US) .build(); List\u0026lt;InvoiceItem\u0026gt; items = new ArrayList\u0026lt;\u0026gt;(); items.add(new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); items.add(new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); Customer customer = new Customer.Builder() .name(\u0026#34;John Doe\u0026#34;) .email(\u0026#34;john.doe@example.com\u0026#34;) .phoneNumber(\u0026#34;555-123-4567\u0026#34;) .build(); Invoice invoice = new Invoice( new InvoiceNumber(\u0026#34;001\u0026#34;), customer, billingAddressm, shippingAddress, LocalDate.now(), items); Amount amount = invoice.getVatAmount(); assertThat(amount).isEqualTo(Amount.EUR(42)) In the case of the aforementioned test, an Invoice object is needed, which, in turn, depends on several other objects. However, it is noteworthy that, for this specific test, only the invoice items hold significance. They play a vital role in asserting and calculating the VAT amount, all the other objects and fields just clutter the readability of the test.\nTest Data - Factory, Generator, Builder, Fixtures \u0026hellip; #There are numerous patterns available to create the required objects for your tests. A very common approach to simplify the creational logic while at the same time having some kind of of code reuse in place is to create static factory methods returning the required objects.\nInvoiceTestDataFactory.invoice(); While this approach may initially appear to solve the creational issue, it tends to scale poorly As Martin Fowler highlights:\nObject Mothers do have their faults. In particular there\u0026rsquo;s a heavy coupling in that many tests will depend on the exact data in the mothers.\nI\u0026rsquo;ve observed several solutions to address this issue, including;\ncreating specific static factory methods adding parameters to differentiate the creational logic. InvoiceTestDataFactory.invoiceWithoutShippingAddress(); InvoiceTestDataFactory.invoiceWithoutBillingAddressAndThreeItems(); InvoiceTestDataFactory.invoice(new InvoiceNumber(\u0026#34;001\u0026#34;), \u0026#34;john@doe,com\u0026#34;); InvoiceTestDataFactory.invoice( new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21)), new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); While these solutions may initially appear to solve the coupling issue, they often result in tight coupling between specific factory methods and the requirements of individual tests, limiting their reusability.\nOne introduces different factory methods or several sets of parameters because there are tests that require different permutations of the same objects under test.\nImagine you have an object with 5 primitive fields and 5 custom-typed fields. The question arises: how many permutations of factory methods or parameter sets would be required to cover all your test cases?\nIntroducing static factory methods or parameters might make the code challenging to maintain and may not facilitate effective communication within the Given part of your tests. Ultimately, pursuing this path leads to test data factories that are difficult to maintain and confusing to use.\nSo, how can we simplify the technical creational logic while simultaneously highlighting its essential aspects?\nEmphasize what matters and hide the irrelevant parts #By combining the Object Mother concept with pre-filled builders it becomes possible to hide away all unnecessary complexity while at the same time emphasizing what matters for your test-case.\nLet\u0026rsquo;s put this into practice using the previous example:\nInvoice invoice = InvoiceMother.invoice() .withInvoiceItems( new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21)), new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))) .build(); Amount amount = invoice.getVatAmount(); assertThat(amount).isEqualTo(Amount.EUR(42)) üëâÔ∏è It\u0026rsquo;s important to stress the prefilled nature of a mother. Calling InvoiceMother.invoice().build() will return a fully fledged Invoice were all fields are filled in containing sensible default values.\nBy utilizing the approach mentioned above, the unnecessary clutter is eliminated, allowing the focus to be solely on what is essential for the test. In this particular case, it becomes evident that having two invoice items priced at EUR 100 each, with a 21% tax applied to each item, results in EUR 42 of taxes\nWe are still using some kind of static factory method, yet it does not immediately return our Invoice rather it returns a builder that allows you to override those defaults that matter for your specific test.\nI like to make the builder part of the Mother class as to reduce the chance to confuse it with production code InvoiceBuilders.\npublic class InvoiceMother { private InvoiceMother() { } public static Builder invoice() { return new Builder(); } public static class Builder { InvoiceNumber invoiceNumber = new InvoiceNumber(\u0026#34;001\u0026#34;); Customer customer = CustomerMother.customer().build(); Address billingAddress = AddressMother.address().build(); Address shippingAddress = AddressMother.address().build(); LocalDate creationDate = LocalDate.now(); List\u0026lt;InvoiceItem\u0026gt; items = List.of(InvoiceItemMother.item().build()); public Builder withItems(List\u0026lt;InvoiceItem\u0026gt; items) { this.items = items; return this; } // other setters are left out for brevity public Invoice build() { return new Invoice( invoiceNumber, customer, billingAddressm, shippingAddress, creationDate, items); } } } It\u0026rsquo;s important to note that the concept discussed here is not about the suffix \u0026lsquo;Mother.\u0026rsquo; If you find the suffix unfavorable, feel free to use any other suffix that better suits your needs, such as InvoiceTestDataFactory, InvoiceFixture, InvoiceTestData, and so on.\nWhat is important:\nLet your factory methods return Builders not the object under creation. Use a pre-filled Builder Limit your static factory methods to the absolute minimum. Rather override a field using the Builder than to introduce a new static factory method. What can flex:\nLet the defaults for custom complex objects depend on other Mothers The Mother suffix Making the Builder part of your Mother class But what if your test requires a specific field in one of the nested custom objects to be in a particular state? For instance, you may need the shipping address\u0026rsquo;s country to be set as \u0026lsquo;US,\u0026rsquo; while the other field values are irrelevant for your test. With the current setup, you would have to pass in another mother object and build it fully changing only that field that matter for your test.\nInvoiceMother.invoice() .withShippingAddress(AddressMother.addres() .withCountry(Country.US) .build()) .build(); Although this approach works, we can further improve it by utilizing a java.util.function.Consumer with the AddressMother.Builder:\nInvoiceMother.invoice() .withShippingAddress(b -\u0026gt; b.withCountry(Country.US)) .build(); In this case, the builder method would look like this:\npublic Builder withShippingAddress(Consumer\u0026lt;AddressMother.Builder\u0026gt; addressConsumer) { Address.Builder builder = AddressMother.address(); addressConsumer.accept(builder) this.shippingAddress = builder.build(); return this; } These enhancements aim to improve the readability and clarity of the code example while retaining the original meaning and intent.\nTakeaways #In conclusion, the Object Mother concept offers developers a powerful approach to effortlessly generate intricate test objects. By combining the concept with pre-filled builders, developers can effectively simplify the technical creational logic while emphasizing the essential aspects of their test cases.\nBy embracing the Object Mother concept and its principles, developers can achieve enhanced code readability, maintainability, and overall testing efficiency in their software projects. This becomes particularly crucial in complex projects, as tests serve not only to verify code correctness and guide design but also to serve as documentation and prevention of regression. When regression does occur, it is vital for both your future self and colleagues to clearly understand what is being tested.\n","date":null,"permalink":"/talks/your-tests-also-need-some-architecting/","section":"My Talks","summary":"\u003cp\u003eDiscover how the Object Mother concept empowers developers to effortlessly generate intricate test objects,\nenhancing code readability, maintainability, and overall testing efficiency.\u003c/p\u003e","title":"Your Tests Also Need Some Architecting"},{"content":"On the religious nature of TDD\nToday I practice a form of Test-Driven Development (TDD) whenever and wherever possible. However, this wasn\u0026rsquo;t always the case. At the beginning of my career, I was oblivious of it. To be honest, I wasn\u0026rsquo;t even focussed on writing tests; I was already content if the production code worked, and only then I would write a test.\nAs much as TDD practitioners would like, the adoption of the methodology, introduced by Kent Beck nearly 20 years ago, remains relatively low1. Conversely, an intriguing paradox emerges when we look beyond the mainstream landscape. TDD remains very much alive, championed by a large group of developers who diligently preach its virtues on social media channels and conferences.\nIn this article series, my goal is to offer a fresh perspective on TDD that goes beyond its common glorification served with oversimplified examples and rigid doctrine. I will provide practical insights to make TDD more recognizable and applicable to the complex challenges developers encounter today.\nNo TDD Never Implied no Tests #Compared to its adoption rate, it is prodigious to see TDD listed in many developer vacancies. Nevertheless, despite my experience spanning more than a decade in diverse teams, companies and projects, I\u0026rsquo;ve rarely encountered developers who wholeheartedly adopted TDD.\nTDD is like a rare bird in the wild, seldom seen but fascinating when it does appear.\nNot applying TDD never implied the absence of tests altogether. In fact, every project I\u0026rsquo;ve worked on has maintained a fair share of tests. Likewise, a significant number of developers with whom I\u0026rsquo;ve collaborated regarded a robust test suite as a cornerstone for the success of their projects. However, what I found to be a rarity was the practice of writing tests first, using these tests to guide the design and engaging in the classic red-green-refactor cycle.\nIn these environments, testing was perceived as a safety net rather than a compass for design. Developers wrote tests after the code was implemented, and the purpose of these tests was primarily to confirm that the code worked as intended and to warn them of regression later on.\nA Valuable Skill or Dogmatic Doctrine? #TDD undeniably played a significant role in putting testing on the map. It had a huge impact on how we develop software today and tomorrow. Nevertheless, it\u0026rsquo;s high time to confront the reality of the dogmatic perception TDD has acquired and cast a more illuminating spotlight on it.\nWhile TDD stands as a valuable methodology for ensuring software quality, I do not blindly want to follow its stringent approach. While its rules2 can helpful, they sometimes inadvertently constrain our approach to problem-solving:\nNot writing production code unless a failing test exists. Not writing more of a test than is necessary to fail. Not writing more production code than is needed to pass the one failing test. TDD is a skill that takes practice and experience to really benefit from it. It cannot be rigorously applied everywhere; experience will teach you this\nMisapplication or overzealous adherence to TDD principles can lead to exactly the opposite of what it might try to achieve.\nIn the world of software development, having a diverse skill set is essential. Therefore, I see TDD as one of the many skills that make up a developer\u0026rsquo;s arsenal, each contributing towards becoming a well-rounded and proficient software engineer.\nEmphasizing the Destination Over the Journey #In practice, I\u0026rsquo;ve learned that the presence of tests is more important than the specific details of how they are written. Whether tests are written using the TDD methodology or not, what truly matters is their role in ensuring the software\u0026rsquo;s correctness, guarding against regressions, serving as documentation and all together allow our software to evolve at a steady pace. While TDD may provide a structured approach, the ultimate goal is maintainable, reliable and well-tested software.\nI firmly advocate attempting to write your tests first, as it\u0026rsquo;s a valuable approach that doesn\u0026rsquo;t necessarily entail the strict adherence to the rules prescribed by traditional TDD.\nExploring the advantages of writing tests first will be our focus in the upcoming sections. However, before looking into that, the next episode will address The Unit Test Ambiguity, as it often hinders discussions about TDD.\nDiffblue. (2020). \u0026ldquo;DevOps and Testing Report.\u0026rdquo; Retrieved from https://www.diffblue.com/DevOps/research_papers/2020-devops-and-testing-report/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTheThreeRulesOfTdd. (2023). http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"14 December 2023","permalink":"/posts/tdd-is-not-a-religion/","section":"Jonas on Software","summary":"\u003cp\u003eOn the religious nature of TDD\u003c/p\u003e","title":"\u003cspan style=\"color: #606a79; font-size: 90%;\"\u003eTest First Ôºç Part 1\u003c/span\u003e\u003cbr/\u003eTDD is Not a Religion"},{"content":"Tests must be encapsulated within methods that demand a name, and as with all aspects of programming, naming often proves to be the most challenging task.\nIn navigating the software landscape, I\u0026rsquo;ve encountered varied approaches to naming test methods. Some teams grant developers the freedom to name tests as they see fit. However, this often leads to a mix of clear, cryptic, misleading, or ambiguous test method names.\nConversely, certain teams enforce test method naming conventions to address these issues. Nevertheless, conventions can feel rigid, clumsy, and unfitting, sometimes resulting in problems similar to having no convention at all.\nThis article embarks on a journey to devise a practical strategy for defining test method names and their structure, all rooted in their inherent value. So, what truly defines the value of a test method name?\nExecutable pieces of documentation #Tests offer an invaluable yet often underestimated asset: their inherent documentation capabilities. As you write tests, you define the boundaries of your business logic or explore the capabilities of your infrastructural code, often without recognizing that you\u0026rsquo;re undertaking an additional vital task ‚Äî documenting the capabilities of your system.\nThe documentation value tests have goes beyond the static nature of traditional documentation found in sources like company wikis. Tests are living pieces of documentation, continuously evolving due to their executable nature. No feature or requirements should be left untested, hence it is documented. And when a test fails, often signalling regression or a change in the required behavior, you are prompted to correct it, ensuring both the accuracy of your system and the correctness of your documentation.\nThe Anatomy of a Superb Name #Conventional wisdom often stipulates that test method names should be concise and comprehensible, yet these directives occasionally lack the depth required to holistically define a well-documented test method name. Here are a few recommendations that I endorse:\nTranscending Standard Java Conventions #Although underscores in method names seldom grace production Java code, I propose that they\u0026rsquo;re an exception in the realm of test code. Test code operates under different rules and priorities, with clarity taking the utmost importance. Since test method names can occasionally become lengthy and words might feel crammed together, incorporating the option to use underscores provides a breathing space that contributes to more readable and comprehensible test names. Additionally, an underscore serves as a perfect separator, creating distinct and clear sections within the method name.\nThis also applies to moving beyond conventional Java method casing rules. Later you\u0026rsquo;ll find some unconventional usages of casing for the sake of readability.\nUbiquitous language #The ubiquitous language, gathered from business lingo and seamlessly integrated into your production code, should seamlessly extend to your test code, including method names.\nMove away from describing: What is being tested #If the essence of a test method name resides in its documentation value, then it\u0026rsquo;s logical to encapsulate the tested behavior within that name. Yet, countless tests merely encapsulate what\u0026rsquo;s being tested, neglecting to describe the behavior. This seemingly subtle distinction wields remarkable power.\nüí°Ask yourself, would the name be clear for a non-technical individual?\nüëé Examples of subpar test names that solely detail what\u0026rsquo;s being tested:\nvoid expireInvoiceTest() { void shouldStartReportJob() { void addPineappleToppingToPizzaTest() { void testCalculateIllegalVATRate() { Glossing over these test names leaves you guessing about the governing rules and expected behavior.\nWhat triggers an invoice\u0026rsquo;s expiration and what follows? What initiates the job and its subsequent outcomes? Is topping a pizza with pineapples allowed? What makes a VAT rate illegal? Avoid giving Examples #When writing a test, even when we are considering the behavior, we often transition from a general definition to a more concrete version. This is perfectly normal, as the test will implement the behavior using actual data or values. However, these example values should not imply their presence in our test method names\nüí°Exclude exemplar data from test method names\nüëé Examples of subpar test names that give examples of describing behavior:\nvoid shouldConsider_abc_asATooShortPassword() void testExpireInvoiceOnAugust15th() { void testReportJobStartsAt8AM() { void testCalculateMinus10PercentVatRate() { It does not describe the general rule which makes a password too short A specific date limits the understanding of the tested behavior to this exact date, lacking generality. Focuses on a particular time, limiting the overal comprehension of when the job should start. Mentions a precise VAT rate, which might not be clear in conveying the overall behavior. The issue with test naming conventions #The value in test method naming conventions often only remains confined to providing structural guidance. These conventions are indeed useful tools, ensuring codebase coherence and aiding new developers in quickly adapting to a consistent naming strategy.\nüí°Ô∏èWithout a primary focus on describing the behavior, conventions can sometimes fall short.\nMost conventions allow you to document the behavioral aspects of your codebase, irrespective of the structure they impose. As we\u0026rsquo;ve explored, a truly effective test method name should encapsulate the behavior, requirement, or feature being tested, though achieving this is often easier said than done.\nDeconstructing Common Conventions #For the remainder of this article, we\u0026rsquo;ll explore commonly found conventions. All examples will be written based on the following fictional requirement:\nüìù An outstanding invoice should incur an automatic 10% fee 30 days after its publication date.\nConvention: must start with should #One commonly encountered convention insists on using a should prefix. While this might naturally spotlight the outcome or the aspect under test, it somewhat sidelines the behavior.\nüëé Lacks complete behavior description:\nvoid shouldAutomaticallyIncur10PercentFee() { üëé Drifts from the ubiquitous language by using:\nadd instead of incur costs instead of fee unpaid instead of outstanding creation instead of publication void shouldAutomaticallAdd10PercentExtraCostsToAnUnpaidInvoice30DaysAfterCreationDate() { üëç Offers a more behavior-centric approach:\nvoid shouldAutomaticallIncur10PercentFeeToAnOutstandingInvoice30DaysAfterPublicationDate() { üëç Emphasizes clarity by sectioning with underscores:\nvoid shouldAutomaticallIncur10PercentFee_ToAnOutstandingInvoice_30DaysAfterPublicationDate() { Convention: Given_state_When_action_Then_outcome #The Given When Then paradigm offers a clear and structured way to capture all aspects of a well-defined behavior. It is well-recognized and used across various domains, making it familiar to developers and non-technical stakeholders alike. However, it\u0026rsquo;s important to note that this convention can become verbose due to the repeated use of Given, When, Then. This structure might lead to lengthy test method names, which could potentially impact readability and maintainability.\nüëç\nvoid Given_OustandingInvoice_When_30DaysAfterPublicationDate_Then_ShouldIncur10PercentFee() { Convention: Givenstate_Whenaction_Thenoutcome #Applying the same convention with a touch of brevity. However, the decision to include underscores ultimately hinges on individual taste and preference.\nüëç\nvoid GivenOustandingInvoice_When30DaysAfterPublicationDate_ThenShouldIncur10PercentFee() { Structural layout #Frequently when testing a requirement, you will need several tests to fully cover it. In this process, these tests can share similar components or structures, resulting in unnecessary redundancy in the test method name. To mitigate the lengthening of test method names, an interesting approach is to abstract these repetitions into distinct code segments. With the advent of JUnit 5, achieving this is feasible through the use of the @Nested annotation, allowing you to segregate the repetitions into separate classes.\nclass InvoiceTest { @Nested class GivenAnOustandingInvoice { @Test void whenPaymentExceeds30DaysLimit_Then_Incur10PercentFee() { @Test void whenWithin30DaysLimit_Then_NoFreeIsIncurred() { } class InvoiceTest { @Nested class ShouldIncur10PercentFee { @Test void whenPaymentExceeds30DaysLimit() { @Test void whenPaymentIsDoneByCreditCard() { However, this approach challenges common conventions, as most conventions primarily focus on method names.\nThrough my own experiences, I\u0026rsquo;ve explored various strategies and reached a definitive realization: while conventions offer structure, they might not inherently capture the intended focus on the behavior under test. It\u0026rsquo;s also worth considering expanding conventions beyond solely method naming and allowing them to be employed within nested classes, providing a holistic approach to enhancing test clarity.\nNo convention will be perfect, but it\u0026rsquo;s important to select one from the beginning and stick with it.\nüí°When selecting a convention, it is essential to recognize that the ultimate objective of a test name is to succinctly encapsulate the intended behavior.\nUpon completing a test, I\u0026rsquo;ve found it beneficial to revisit the test method itself and contemplate the following question:\nüí° If I were to revisit this test within a year or if a new colleague were to read it, would the name effectively convey the behavior without needing a deep dive into the implementation details?\n","date":"27 August 2023","permalink":"/posts/subtle-art-of-java-test-method-naming/","section":"Jonas on Software","summary":"\u003cp\u003eTests must be encapsulated within methods that demand a name, and as with all aspects of programming, naming often proves to be the most challenging task.\u003c/p\u003e","title":"The subtle Art of Java Test Method Naming"},{"content":"Explore strategies for reliable testing of time-dependent code, including techniques to mitigate flakiness in tests and enable precise time manipulation within your test suites.\nBy default, Java relies on the system clock to determine the current date and time. While in most cases this approach works fine when our system clock remains unchanged, it can quickly turn into a nightmare upon a simple faulty configuration change outside of your control.\nI have first-hand witnessed the consequences of an innocent OS upgrade silently altering the default time zone of a system. The impact was nothing short of disastrous, as all timestamps generated and stored by the application were suddenly off by a couple of hours. This post delves into the root cause of the issue, and as we will see, it presents a simple solution that at the same time enables us to write more robust tests and gain better control over the time-related aspects of our Java applications.\nThe time is now #The culprit of having our code depend on the system clock is the use of static java.time...now() methods such as:\nLocalDate.now() LocalDateTime.now() ZonedDateTime.now() OffsetDateTime.now() Instant.now() Examining the implementation of the ZonedDateTime.now() method in the JDK, we can see it delegates to another now method that takes a java.time.Clock instance as a parameter.\npublic final class ZonedDateTime implements Temporal, ChronoZonedDateTime\u0026lt;LocalDate\u0026gt;, Serializable { public static LocalTime now() { return now(Clock.systemDefaultZone()); } public static ZonedDateTime now(Clock clock) { Objects.requireNonNull(clock, \u0026#34;clock\u0026#34;); final Instant now = clock.instant(); // called once return ofInstant(now, clock.getZone()); } Within java.time.clock we can find a solution for ensuring our time related code is independent of the system\u0026rsquo;s time zone, while also facilitating easier testing. To further clarify this point, let\u0026rsquo;s refer to an excerpt from its documentation:\nUse of a Clock is optional. All key date-time classes also have a now() factory method that uses the system clock in the default time zone. The primary purpose of this abstraction is to allow alternate clocks to be plugged in as and when required. Applications use an object to obtain the current time rather than a static method. This can simplify testing.\nJava documentation\nThink of java.time.Clock as a physical wall clock that can be easily replaced with another clock within a different time zone. By passing along a java.time.Clock in all the java.time...now() methods, we decouple date-time generation from the system\u0026rsquo;s clock and simultaneously make it easier to test. As also stated in the documentation of the ZonedDateTime.now(Clock clock) method:\nUsing this method allows the use of an alternate clock for testing. The alternate clock may be introduced using dependency injection.\nJava documentation\nControlling the correct time zone #Passing a clock bound to a specific time zone frees us from relying solely on the system\u0026rsquo;s clock for date-time generation.\nZonedDateTime.now(Clock.system(ZoneId.of(\u0026#34;Europe/Brussels\u0026#34;))) By introducing a clock dependency, our production code becomes an area where accessing java.time.Clock is necessary. The same requirement extends to our tests, offering us significant benefits. In the next section, we will focus on the tests first and then proceed to examine our production code.\nFixating the clock #Having fine-grained control over date-time generation in our tests is indispensable for avoiding flaky tests. Java helps by allowing us to fixate the clock on a certain date-time within a specific time zone:\nClock.fixed(Instant.parse(\u0026#34;1985-02-25T23:00:00.00Z\u0026#34;), ZoneId.of(\u0026#34;Europe/Brussels\u0026#34;)); Let\u0026rsquo;s take the following piece of production code we want to test:\npublic class Order { private Status status; private LocalDateTime processDateTime; public Order markAsProcessed() { this.processDateTime = LocalDateTime.now(); this.status = PROCESSED; } } By adding a java.time.Clock as a dependency this becomes easily and accurately testsable:\npublic Order markAsProcessed(Clock clock) { this.processDateTime = LocalDateTime.now(clock); Which would result in the following test:\n// given var order = OrderMother.newOrder(); // when var clock = Clock.fixed(Instant.parse(\u0026#34;1985-02-25T23:00:00.00Z\u0026#34;), ZoneId.of(\u0026#34;Europe/Brussels\u0026#34;)); order.markAsProcessed(clock); // then assertThat(order.processDateTime()) .isEqualTo(LocalDateTime.parse(\u0026#34;1985-02-26T00:00:00\u0026#34;)); Notice how in the assertion the day has moved on by one hour and one day, because the date-time is generated in a +1 time zone.\nAlternative solutions #Truncating time for more precision #There are alternative solutions available for testing date-time generation if you prefer not to rely on java.time.Clock. However, it\u0026rsquo;s important to note that relying solely on the system\u0026rsquo;s default clock carries its own risks.\nThe problem with asserting the generated date-time is that even a small amount of time between generating the date-time and asserting it can lead to faulty assertions. Consequently, this test example without the clock will likely be unreliable:\n// given var order = OrderMother.newOrder(); // when order.markAsProcessed(); // then assertThat(order.processDateTime()) .isEqualTo(LocalDateTime.now()); This test is prone to flakiness and will probably fail consistently due to the time that elapses between the markAsProcessed method\u0026rsquo;s LocalDateTime.now() call and the actual assertion. To mitigate this, you can truncate the generated date-time to seconds or milliseconds and perform the same truncation in the assertion. While this approach reduces flakiness, it doesn\u0026rsquo;t guarantee 100% accuracy. But in most cases that trade-off is acceptable. Here\u0026rsquo;s an example of truncating to seconds in your production code:\nInstant.now().truncatedTo(ChronoUnit.SECONDS); And this can be tested like so:\n// given var order = OrderMother.newOrder(); // when order.markAsProcessed(); // then assertThat(order.processDateTime()) .isEqualTo(LocalDateTime.now().truncatedTo(ChronoUnit.SECONDS)); Wondering what this OrderMother is, check out my previous article on mothers\nAssert that the generated time is close enough to now #A second alternative is to utilize AssertJ\u0026rsquo;s .closeTo assertion methods, which provides a convenient way to assert values within a specified range. Here are a couple of examples to illustrate this:\nvar localDateTime = LocalDateTime.now(Clock.systemUTC()); assertThat(localDateTime) .isCloseToUtcNow(within(1, ChronoUnit.SECONDS)); var instant = Instant.parse(\u0026#34;2000-01-01T00:00:00.00Z\u0026#34;); assertThat(instant) .isCloseTo(\u0026#34;1999-12-31T23:59:59.99Z\u0026#34;, within(10, ChronoUnit.MILLIS)); Clock as a spring bean #Going forward with the clock. Whenever we require access to the current date-time, it is necessary to have access to the clock. However, passing the clock object throughout our code can become cumbersome. Fortunately, most modern applications make use of an IoC (Inversion of Control) Container, which alleviates this burden. As a result, we will expose the Clock as an object eligible for inversion of control or, in Spring terminology, convert it into a bean.\n@Configuration class ClockConfiguration { @Bean Clock clock(@Value(\u0026#34;${app.time.zone-id}\u0026#34;) String zoneId){ return Clock.system(ZoneId.of(zoneId)); } } Depending on our needs, we have the option to either hard-code the active timezone or, as demonstrated in the example above, make it configurable. In either case, this approach allows us to conveniently inject the clock whenever necessary, as illustrated in the example below.\n@Service public class OrderService { private final Clock clock; private final OrderRepository orderRepository; public OrderService(OrderRepository orderRepository, Clock clock) { this.clock = clock; this.orderRepository = orderRepository; } public void processOrder(OrderId orderId) { var order = orderRepository.findById(orderId); order.markAsProcessed(clock); // etc .. } } Particularly within testing scenarios, having a clock eligible for inversion of control becomes critical. As it empowers us to precisely control and manipulate time-related behavior in our tests. Just like in our unit tests, let\u0026rsquo;s start by exposing a fixated clock.\n@Configuration public class TestClockConfiguration { @Bean @Primary Clock fixedClock() { return Clock.fixed(Instant.parse(\u0026#34;1985-02-25T23:00:00.00Z\u0026#34;), ZoneId.of(\u0026#34;Europe/Brussels\u0026#34;)); } } To ensure consistent time behavior across all tests within a Spring context, we can include the above configuration in our test package. This will, due to the use of @Primary override the clock defined in our production code with a fixed clock. Now we can assert the order to be marked as processed at the fixed date-time.\nWhat if we require precise control over the clock at a per-test level within a single Spring context, without the need to create a new context for each case where a different clock is desired? Or maybe we want to play with our current date-time and actually move time forward or maybe even rewind it? The next section will cover these use-cases.\nMutable clock #The default java.time.Clock implementation is immutable in the sense that you can not change it\u0026rsquo;s current date-time or timezone. By incorporating a mutable clock that can manipulate time or be set to a specific date-time within our tests, we can avoid the need for multiple Spring contexts.\npublic class MutableClock extends Clock { private Instant instant; private final ZoneId zone; public MutableClock(Instant instant, ZoneId zone) { this.instant = instant; this.zone = zone; } @Override public ZoneId getZone() { return zone; } @Override public Clock withZone(ZoneId zone) { return new MutableClock(instant, zone); } @Override public Instant instant() { return instant; } public void fastForward(TemporalAmount temporalAmount) { set(instant().plus(temporalAmount)); } public void rewind(TemporalAmount temporalAmount) { set(instant().minus(temporalAmount)); } public void set(Instant instant) { this.instant = instant; } public static MutableClock fixed(Instant instant, ZoneId zone) { return new MutableClock(instant, zone); } public static MutableClock fixed(OffsetDateTime offsetDateTime) { return fixed(offsetDateTime.toInstant(), offsetDateTime.getOffset()); } } Exposing the mutable clock:\n@Configuration public class TestClockConfiguration { @Bean @Primary Clock testClock(@Value(\u0026#34;${app.time.zone-id}\u0026#34;) String zoneId) { return new MutableClock(Instant.parse(\u0026#34;1985-02-25T23:00:00.00Z\u0026#34;), ZoneId.of(zoneId)); } } Now, let\u0026rsquo;s put this knowledge to good use by writing a test for the following piece of production code:\n@Component public class OrderProcessor { private final Clock clock; private final LocalTime startOfWorkingDay = LocalTime.of(8, 0); private final LocalTime endOfWorkingDay = LocalTime.of(22, 0); public OrderProcessor(Clock clock) { this.clock = clock; } public void processOrder(Order order) { if (isWithinWorkingHours()) { processNow(order); } else { processLater(order); } } public boolean isWithinWorkingHours() { LocalTime now = LocalTime.now(clock); return !now.isBefore(startOfWorkingDay) \u0026amp;\u0026amp; !time.isAfter(endOfWorkingDay); } } public class OrderProcessingFeatureTest { private final MutableClock mutableClock; public OrderProcessingFeatureTest(Clock clock) { this.mutableClock = (MutableClock)clock; } @Test void shouldProcessOrderWithinWorkingHours() { // given mutableClock.set(Instant.parse(\u0026#34;2023-02-25T13:00:00.00Z\u0026#34;)); // when var resultActions = mockMvc.perform(post(\u0026#34;/orders/ORD567890\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(toJson(OrderMother.defaultOrder()))); // then resultActions.andExpect(status().isOk()) .andExpect(jsonPath(\u0026#34;$.status\u0026#34;).value(\u0026#34;PROCESSED\u0026#34;)); } @Test void shouldProcessOrderLaterWhenReceivedOutsideOfWorkingHours() { // given mutableClock.set(Instant.parse(\u0026#34;2023-02-25T23:00:00.00Z\u0026#34;)); // when var resultActions = mockMvc.perform(post(\u0026#34;/orders/ORD4785669\u0026#34;) .contentType(MediaType.APPLICATION_JSON) .content(toJson(OrderMother.defaultOrder()))); // then resultActions.andExpect(status().isOk()) .andExpect(jsonPath(\u0026#34;$.status\u0026#34;).value(\u0026#34;PROCESS_LATER\u0026#34;)); } } In conclusion, leveraging the java.time.Clock class and its capability to decouple date-time generation from the system clock empowers us to write more effective tests for time-dependent code. This approach not only grants us greater control over time-related aspects but also acts as a safeguard against issues arising from system clock changes, as I have personally encountered.\n","date":"9 July 2023","permalink":"/posts/how-to-effectively-test-time-dependent-code/","section":"Jonas on Software","summary":"\u003cp\u003eExplore strategies for reliable testing of time-dependent code, including techniques to mitigate flakiness in tests and enable precise time manipulation within your test suites.\u003c/p\u003e","title":"How to Effectively Test Time-Dependent Code: Unit and Spring-Based Strategies"},{"content":"Discover how the Object Mother concept empowers developers to effortlessly generate intricate test objects, enhancing code readability, maintainability, and overall testing efficiency.\nThe creational problem #A good structured test exists out of three parts commonly known as: Given When Then or Arrange Act Assert.\nWithin the Given part you declare a set of objects that will drive your test. Initially the creational logic is simple, as the objects may have fewer fields or lack coherence. However, inevitably over time, the set of objects grows as their intricacy, they become more complex and time-consuming to construct.\nVarious solutions exist to address those issues, such as Test Data Factories, Data Fakers, Test Data Generators or Fixture Builders etc .. In the end they all share a common goal: simplifying object creation while ensuring reusability. Martin Fowler has even coined a term for this concept, known as Object Mother.\nLet\u0026rsquo;s delve deeper into the problem by examining a concrete example. While this example may not be overly complex or entirely realistic, the purpose of this article is to work with practical scenarios without getting bogged down in the intricacies of object creation.\nAddress billingAddress = new Address.Builder() .streetAddress(\u0026#34;123 Main St\u0026#34;) .city(\u0026#34;Anytown\u0026#34;) .state(\u0026#34;CA\u0026#34;) .zipCode(\u0026#34;12345\u0026#34;) .country(Country.US) .build(); Address shippingAddress = new Address.Builder() .streetAddress(\u0026#34;456 Oak Ave\u0026#34;) .city(\u0026#34;Othertown\u0026#34;) .state(\u0026#34;CA\u0026#34;) .zipCode(\u0026#34;67890\u0026#34;) .country(Country.US) .build(); List\u0026lt;InvoiceItem\u0026gt; items = new ArrayList\u0026lt;\u0026gt;(); items.add(new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); items.add(new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); Customer customer = new Customer.Builder() .name(\u0026#34;John Doe\u0026#34;) .email(\u0026#34;john.doe@example.com\u0026#34;) .phoneNumber(\u0026#34;555-123-4567\u0026#34;) .build(); Invoice invoice = new Invoice( new InvoiceNumber(\u0026#34;001\u0026#34;), customer, billingAddressm, shippingAddress, LocalDate.now(), items); Amount amount = invoice.getVatAmount(); assertThat(amount).isEqualTo(Amount.EUR(42)) In the case of the aforementioned test, an Invoice object is needed, which, in turn, depends on several other objects. However, it is noteworthy that, for this specific test, only the invoice items hold significance. They play a vital role in asserting and calculating the VAT amount, all the other objects and fields just clutter the readability of the test.\nTest Data - Factory, Generator, Builder, Fixtures \u0026hellip; #There are numerous patterns available to create the required objects for your tests. A very common approach to simplify the creational logic while at the same time having some kind of of code reuse in place is to create static factory methods returning the required objects.\nInvoiceTestDataFactory.invoice(); While this approach may initially appear to solve the creational issue, it tends to scale poorly As Martin Fowler highlights:\nObject Mothers do have their faults. In particular there\u0026rsquo;s a heavy coupling in that many tests will depend on the exact data in the mothers.\nI\u0026rsquo;ve observed several solutions to address this issue, including;\ncreating specific static factory methods adding parameters to differentiate the creational logic. InvoiceTestDataFactory.invoiceWithoutShippingAddress(); InvoiceTestDataFactory.invoiceWithoutBillingAddressAndThreeItems(); InvoiceTestDataFactory.invoice(new InvoiceNumber(\u0026#34;001\u0026#34;), \u0026#34;john@doe,com\u0026#34;); InvoiceTestDataFactory.invoice( new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21)), new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))); While these solutions may initially appear to solve the coupling issue, they often result in tight coupling between specific factory methods and the requirements of individual tests, limiting their reusability.\nOne introduces different factory methods or several sets of parameters because there are tests that require different permutations of the same objects under test.\nImagine you have an object with 5 primitive fields and 5 custom-typed fields. The question arises: how many permutations of factory methods or parameter sets would be required to cover all your test cases?\nIntroducing static factory methods or parameters might make the code challenging to maintain and may not facilitate effective communication within the Given part of your tests. Ultimately, pursuing this path leads to test data factories that are difficult to maintain and confusing to use.\nSo, how can we simplify the technical creational logic while simultaneously highlighting its essential aspects?\nEmphasize what matters and hide the irrelevant parts #By combining the Object Mother concept with pre-filled builders it becomes possible to hide away all unnecessary complexity while at the same time emphasizing what matters for your test-case.\nLet\u0026rsquo;s put this into practice using the previous example:\nInvoice invoice = InvoiceMother.invoice() .withInvoiceItems( new InvoiceItem(\u0026#34;Product A\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21)), new InvoiceItem(\u0026#34;Product B\u0026#34;, Amount.EUR(100.00), Tax.vatPercentage(21))) .build(); Amount amount = invoice.getVatAmount(); assertThat(amount).isEqualTo(Amount.EUR(42)) üëâÔ∏è It\u0026rsquo;s important to stress the prefilled nature of a mother. Calling InvoiceMother.invoice().build() will return a fully fledged Invoice were all fields are filled in containing sensible default values.\nBy utilizing the approach mentioned above, the unnecessary clutter is eliminated, allowing the focus to be solely on what is essential for the test. In this particular case, it becomes evident that having two invoice items priced at EUR 100 each, with a 21% tax applied to each item, results in EUR 42 of taxes\nWe are still using some kind of static factory method, yet it does not immediately return our Invoice rather it returns a builder that allows you to override those defaults that matter for your specific test.\nI like to make the builder part of the Mother class as to reduce the chance to confuse it with production code InvoiceBuilders.\npublic class InvoiceMother { private InvoiceMother() { } public static Builder invoice() { return new Builder(); } public static class Builder { InvoiceNumber invoiceNumber = new InvoiceNumber(\u0026#34;001\u0026#34;); Customer customer = CustomerMother.customer().build(); Address billingAddress = AddressMother.address().build(); Address shippingAddress = AddressMother.address().build(); LocalDate creationDate = LocalDate.now(); List\u0026lt;InvoiceItem\u0026gt; items = List.of(InvoiceItemMother.item().build()); public Builder withItems(List\u0026lt;InvoiceItem\u0026gt; items) { this.items = items; return this; } // other setters are left out for brevity public Invoice build() { return new Invoice( invoiceNumber, customer, billingAddressm, shippingAddress, creationDate, items); } } } It\u0026rsquo;s important to note that the concept discussed here is not about the suffix \u0026lsquo;Mother.\u0026rsquo; If you find the suffix unfavorable, feel free to use any other suffix that better suits your needs, such as InvoiceTestDataFactory, InvoiceFixture, InvoiceTestData, and so on.\nWhat is important:\nLet your factory methods return Builders not the object under creation. Use a pre-filled Builder Limit your static factory methods to the absolute minimum. Rather override a field using the Builder than to introduce a new static factory method. What can flex:\nLet the defaults for custom complex objects depend on other Mothers The Mother suffix Making the Builder part of your Mother class But what if your test requires a specific field in one of the nested custom objects to be in a particular state? For instance, you may need the shipping address\u0026rsquo;s country to be set as \u0026lsquo;US,\u0026rsquo; while the other field values are irrelevant for your test. With the current setup, you would have to pass in another mother object and build it fully changing only that field that matter for your test.\nInvoiceMother.invoice() .withShippingAddress(AddressMother.address() .withCountry(Country.US) .build()) .build(); Although this approach works, we can further improve it by utilizing a java.util.function.Consumer with the AddressMother.Builder:\nInvoiceMother.invoice() .withShippingAddress(b -\u0026gt; b.withCountry(Country.US)) .build(); In this case, the builder method would look like this:\npublic Builder withShippingAddress(Consumer\u0026lt;AddressMother.Builder\u0026gt; addressConsumer) { Address.Builder builder = AddressMother.address(); addressConsumer.accept(builder) this.shippingAddress = builder.build(); return this; } These enhancements aim to improve the readability and clarity of the code example while retaining the original meaning and intent.\nTakeaways #In conclusion, the Object Mother concept offers developers a powerful approach to effortlessly generate intricate test objects. By combining the concept with pre-filled builders, developers can effectively simplify the technical creational logic while emphasizing the essential aspects of their test cases.\nBy embracing the Object Mother concept and its principles, developers can achieve enhanced code readability, maintainability, and overall testing efficiency in their software projects. This becomes particularly crucial in complex projects, as tests serve not only to verify code correctness and guide design but also to serve as documentation and prevention of regression. When regression does occur, it is vital for both your future self and colleagues to clearly understand what is being tested.\n","date":"5 June 2023","permalink":"/posts/object-mother/","section":"Jonas on Software","summary":"\u003cp\u003eDiscover how the Object Mother concept empowers developers to effortlessly generate intricate test objects,\nenhancing code readability, maintainability, and overall testing efficiency.\u003c/p\u003e","title":"Mastering the Object Mother"},{"content":"After reading the article I believe it contains a fatal flaw, namely testing implementation details. Because that is what you are doing if you are focused on testing classes and code. You do not test the code you test the behavior and through doing that you test the code. So you need to look for the API (as in interface or gate) that leads to that behavior. Within the refactoring step that interface should never change unless you misinterpreted the behavior you are implementing. Hence this allows you to move things around as freely as possible during the refactoring phase while not breaking your tests.\nIf you are testing classes you are often forced to use mocks and it will not allow you to refactor freely.\nI do agree with the sentiment that TDD is over evangelized, it is also very difficult and takes a lot of knowledge in general, like stated in the article, on software design and patterns.\n@Test @WithMockUser void linkBudgetWithBmatPlaylistShouldReturn202() throws Exception { // given var budgetId = UUID.randomUUID(); var playlistLinkDTO = new PlaylistLinkDTO(PlaylistSource.BMAT, List.of(new LinkablePlaylist(1, \u0026#34;df\u0026#34;), new LinkablePlaylist(2,\u0026#34;df\u0026#34;))); // when this.mvc.perform(post(\u0026#34;/budgets/{budget-id}/link\u0026#34;, budgetId).with(csrf()) .contentType(MediaType.APPLICATION_JSON) .content(objectMapper.writeValueAsString(playlistLinkDTO))) .andExpect(status().isAccepted()); // then Mockito.verify(b1ntvService).linkB1ntvWithBmatPlaylists(Mockito.eq(budgetId), Mockito.argThat(actual -\u0026gt; actual.containsAll(playlistLinkDTO.getLinkedPlaylists()))); } Can\u0026rsquo;t move on unless.I update the service because it has changed\n","date":"1 January 0001","permalink":"/posts/almost-never-mocking/","section":"Jonas on Software","summary":"","title":""},{"content":"shouldSendMessageToDlq_whenWrongMessageIsSent vs dlqDataIntegrityViolationExceptions\n","date":"1 January 0001","permalink":"/posts/new-insights/","section":"Jonas on Software","summary":"","title":""},{"content":"upon every async step -\u0026gt; verify it has been executed correctly even if this is not your final assert or what you want to test\n","date":"1 January 0001","permalink":"/posts/testing-async-code/","section":"Jonas on Software","summary":"","title":""},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" üìö Some noteworthy projects I\u0026rsquo;ve created along the way. Bob #Builder generator for Java\nüè° homepage: https://github.com/jonas-grgt/bob\nUsage #@Buildable class Car { private final String color; private final Make make; public Car(String color, Make make) { this.color = color; this.make = make; } } Xjx #Java based XML serializing and deserializing (serdes) library.\nüè° homepage: https://github.com/jonas-grgt/xjx\nExample deserialization usage: #class Gpx { @Tag(path = \u0026#34;/gpx\u0026#34;, items = \u0026#34;wpt\u0026#34;) List\u0026lt;Wpt\u0026gt; wpts; } class Wpt { @Tag(path = \u0026#34;/gpx/wpt/name\u0026#34;) String name; } var gpx = new XjxSerdes().read(\u0026#34;\u0026#34;\u0026#34; \u0026lt;gpx version=\u0026#34;1.0\u0026#34; creator=\u0026#34;ExpertGPS 1.1 - https://www.topografix.com\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns=\u0026#34;http://www.topografix.com/GPX/1/0\u0026#34; xsi:schemaLocation=\u0026#34;http://www.topografix.com/GPX/1/0 http://www.topografix.com/GPX/1/0/gpx.xsd\u0026#34;\u0026gt; \u0026lt;time\u0026gt;2002-02-27T17:18:33Z\u0026lt;/time\u0026gt; \u0026lt;bounds minlat=\u0026#34;42.401051\u0026#34; minlon=\u0026#34;-71.126602\u0026#34; maxlat=\u0026#34;42.468655\u0026#34; maxlon=\u0026#34;-71.102973\u0026#34;/\u0026gt; \u0026lt;wpt lat=\u0026#34;42.438878\u0026#34; lon=\u0026#34;-71.119277\u0026#34;\u0026gt; \u0026lt;ele\u0026gt;44.586548\u0026lt;/ele\u0026gt; \u0026lt;time\u0026gt;2001-11-28T21:05:28Z\u0026lt;/time\u0026gt; \u0026lt;name\u0026gt;5066\u0026lt;/name\u0026gt; \u0026lt;desc\u0026gt;\u0026lt;![CDATA[5066]]\u0026gt;\u0026lt;/desc\u0026gt; \u0026lt;sym\u0026gt;Crossing\u0026lt;/sym\u0026gt; \u0026lt;type\u0026gt;\u0026lt;![CDATA[Crossing]]\u0026gt;\u0026lt;/type\u0026gt; \u0026lt;/wpt\u0026gt; \u0026lt;/gpx\u0026gt; \u0026#34;\u0026#34;\u0026#34;, Gpx.class); ","date":null,"permalink":"/my-open-code-overture/","section":"","summary":"","title":"My Open Code Overture"},{"content":"","date":null,"permalink":"/tags/opensource/","section":"Tags","summary":"","title":"Opensource"},{"content":"","date":null,"permalink":"/tags/projects/","section":"Tags","summary":"","title":"Projects"}]